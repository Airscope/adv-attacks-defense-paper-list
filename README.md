# Adversarial Attacks and Defense Paper Collections

## Papers

### :star2: Paper list collections:star2:

- **A Paper list of Adversarial Attack on Object Detection:** [[paper-list](https://github.com/idrl-lab/Adversarial-Attacks-on-Object-Detectors-Paperlist)] 

- **awesome-3D-point-cloud-attacks:**

  [[paper-list](https://github.com/cuge1995/awesome-3D-point-cloud-attacks)]

- **Backdoor Learning Resources:**

  [[paper-list](https://github.com/THUYimingLi/backdoor-learning-resources)]

- **Awesome Data Poisoning and Backdoor Attacks:**

  [[paper-list](https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attacks)]

- **A Paper List for Localized Adversarial Patch Research:**

  [[paper-list](https://github.com/inspire-group/adv-patch-paper-list)]

- **Awesome Graph Adversarial Learning:**

  [[paper-list](https://github.com/EdisonLeeeee/Graph-Adversarial-Learning)]

- **Papers-of-Robust-ML:**

  [[paper-list](https://github.com/P2333/Papers-of-Robust-ML)]

- **Learning-with-Noisy-Labels:**

  [[paper-list](https://github.com/weijiaheng/Advances-in-Label-Noise-Learning)]

- [对抗攻击与防御（2022年顶会顶刊AAAI、ACM、 ECCV、NIPS、ICLR、CVPR）adversarial attack and defense汇总](http://e.betheme.net/article/show-1332684.html?action=onClick)

- [NeurIPS2022|对抗攻防论文整理](https://zhuanlan.zhihu.com/p/588881767)

- [NeurIPS2022 Adversarial/Backdoor Robustness 相关论文](https://zhuanlan.zhihu.com/p/591547851)

---



### AAAI 2022

#### Attack

- **BSC-Attack:** Kai Chen, Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang.<br />

  "Attacking Video Recognition Models with Bullet-Screen Comments." AAAI(2022)

  [[paper](https://arxiv.org/pdf/2110.15629.pdf)]

  [[code](https://github.com/kay-ck/BSC-attack)]

- **FCA:** Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Zhiqiang Gong, Xiaoya Zhang, Wen Yao, Xiaoqian Chen.<br />

  "FCA: Learning a 3D Full-Coverage Vehicle Camouflage for Multi-View Physical Adversarial Attack." AAAI(2022)

  [[paper](https://arxiv.org/abs/2109.07193v3)]

  [[code](https://idrl-lab.github.io/Full-coverage-camouflage-adversarial-attack/)]

  [[code](https://forge.osredm.com/projects/p58074962/Full-coverage-camouflage-adversarial-attack)]

  [[code](https://github.com/idrl-lab/Full-coverage-camouflage-adversarial-attack)]

- **TT:** Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang.<br />

  "Boosting the Transferability of Video Adversarial Examples via Temporal Translation." AAAI(2022)

  [[paper](https://arxiv.org/pdf/2110.09075.pdf)]

  [[code](https://github.com/zhipeng-wei/TT)]

- **PNA-PatchOut:** Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang.<br />
  "Towards Transferable Adversarial Attacks on Vision Transformers." AAAI(2022)

  [[paper](https://arxiv.org/pdf/2109.04176.pdf)]

  [[code](https://github.com/zhipeng-wei/PNA-PatchOut)]

- **RoHe:** Mengmei Zhang, Xiao Wang, Meiqi Zhu, Chuan Shi, Zhiqiang Zhang, Jun Zhou.<br />
  "Robust Heterogeneous Graph Neural Networks against Adversarial Attacks."

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20357/20116)]

  [[code](https://github.com/BUPT-GAMMA/RoHe)]

- **robustgraph:**  Xu, Yang Yang, Junru Chen, Xin Jiang, Chunping Wang, Jiangang Lu, Yizhou Sun.

  "Unsupervised Adversarially Robust Representation Learning on Graphs."

  [[paper](https://arxiv.org/abs/2012.02486)]

  [[code](https://github.com/galina0217/robustgraph)]

- **AT-BMC:** Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang.
  "Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction."

  [[paper](https://arxiv.org/abs/2112.10424)]

  [[code](https://github.com/crazyofapple/AT-BMC)]

- **LLTA:** Shuman Fang, Jie Li, Xianming Lin, Rongrong Ji.

  "Learning to Learn Transferable Attack."

  [[paper](https://arxiv.org/abs/2112.06658)]

  [[code](https://github.com/fangshuman/LLTA)]

- **Sparse-RS:** Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas Flammarion, Matthias Hein.

  "Sparse-RS: A Versatile Framework for Query-Efficient Sparse Black-Box Adversarial Attacks."

  [[paper](https://arxiv.org/abs/2006.12834)]

  [[code](https://github.com/fra31/sparse-rs)]

- **SPGA:** Zhenbo Shi, Zhi Chen, Zhenbo Xu, Wei Yang, Zhidong Yu, Liusheng Huang.

  "Shape Prior Guided Attack: Sparser Perturbations on 3D Point Clouds. "

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/download/20802/20561)]

- Wooju Lee, Hyun Myung:
  "Adversarial Attack for Asynchronous Event-Based Data."

  [[paper](https://arxiv.org/abs/2112.13534)]

- **CLPA:** Bingyin Zhao, Yingjie Lao:

  "CLPA: Clean-Label Poisoning Availability Attacks Using Generative Adversarial Nets."

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20902/20661)]

  [[code]( https://github.com/bxz9200/CLPA.)]

- **TextHoaxer:** Muchao Ye, Chenglin Miao, Ting Wang, Fenglong Ma:

  "TextHoaxer: Budgeted Hard-Label Adversarial Attacks on Text."

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/download/20303/20062)]

  [[code](https://github.com/machinelearning4health/TextHoaxer)]

- Rui Ning, Jiang Li, Chunsheng Xin, Hongyi Wu, Chonggang Wang:
  "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks."

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/download/21272/21021)]

- Neil G. Marchant, Benjamin I. P. Rubinstein, Scott Alfeld:

  "Hard to Forget: Poisoning Attacks on Certified Machine Unlearning."

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20736/20495)]

  [[code](https://github.com/ngmarchant/attack-unlearning)]

* Zikui Cai, Xinxin Xie, Shasha Li, Mingjun Yin, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury, M. Salman Asif:
  Context-Aware Transfer Attacks for Object Detection.

  [[paper](https://arxiv.org/abs/2112.03223)]

  [[code](https://github.com/CSIPlab/context-aware-attacks)]

* Xinjian Luo, Xiaokui Xiao, Yuncheng Wu, Juncheng Liu, Beng Chin Ooi:

  A Fusion-Denoising Attack on InstaHide with Data Augmentation. 

  [[paper](https://arxiv.org/abs/2105.07754)]

  [[code](https://github.com/xinjianluo/FDN)]

* Shihong Fang, Anna Choromanska:

  Backdoor Attacks on the DNN Interpretation System.

  [[paper](https://arxiv.org/abs/2011.10698)]

  [[code]()]

* Jiarong Xu, Yizhou Sun, Xin Jiang, Yanhao Wang, Chunping Wang, Jiangang Lu, Yang Yang:

  Blindfolded Attackers Still Threatening: Strict Black-Box Adversarial Attacks on Graphs.

  [[paper](https://arxiv.org/abs/2012.06757)]

  [[code](https://github.com/galina0217/stack)]

* Yibing Du, Antoine Bosselut, Christopher D. Manning:

  Synthetic Disinformation Attacks on Automated Fact Verification Systems.

  [[paper](https://arxiv.org/abs/2202.09381)]

  [[code](https://github.com/Yibing-Du/adversarial-factcheck)]

* Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto:

  Adversarial Bone Length Attack on Action Recognition.

  [[paper](https://arxiv.org/abs/2109.05830)]

  [[code]()]

* Kartik Gupta, Thalaiyasingam Ajanthan:
  Improved Gradient-Based Adversarial Attacks for Quantized Networks.

  [[paper](https://arxiv.org/abs/2003.13511)]

  [[code](https://github.com/kartikgupta-at-anu/attack-bnn)]

* Anshuka Rangi, Long Tran-Thanh, Haifeng Xu, Massimo Franceschetti:

  Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification.

  [[paper](https://arxiv.org/abs/2102.07711)]

* Yunhe Feng, Chirag Shah:
  Has CEO Gender Bias Really Been Fixed? Adversarial Attacking and Improving Gender Fairness in Image Search

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/download/21445/21194)]

  [[code](https://github.com/YunheFeng/CIRF)]

* Maosen Li, Yanhua Yang, Kun Wei, Xu Yang, Heng Huang:

  Learning Universal Adversarial Perturbation by Adversarial Example.

  [[paper](https://arxiv.org/abs/1708.05207)]

  [[code](https://github.com/jhayes14/UAN)]

* Junhua Zou, Yexin Duan, Boyu Li, Wu Zhang, Yu Pan, Zhisong Pan:
  Making Adversarial Examples More Transferable and Indistinguishable.

  [[paper](https://arxiv.org/abs/2007.03838)]

  [[code](https://github.com/278287847/AI-FGTM)]

- Sayak Paul, Pin-Yu Chen:
  Vision Transformers Are Robust Learners.

  [[paper](https://arxiv.org/abs/2105.07581)]

  [[code](https://git.io/J3VO0.)]

#### Defense

- Jinyuan Jia, Yupei Liu, Xiaoyu Cao, Neil Zhenqiang Gong:

  Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks.

  [[paper](https://arxiv.org/abs/2012.03765)]

- Seungyong Moon, Gaon An, Hyun Oh Song:

  Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks.

  [[paper](https://arxiv.org/abs/2112.05634)]

  [[code](https://github.com/snu-mllab/preemptive-robustification)]

- Mingyu Guo, Jialiang Li, Aneta Neumann, Frank Neumann, Hung Nguyen:

  Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs. 

  [[paper](https://arxiv.org/abs/2112.13175)]

- Thanh Nguyen, Haifeng Xu:

  When Can the Defender Effectively Deceive Attackers in Security Games?

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/21172/20921)]

- Hanjie Chen, Yangfeng Ji:
  Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation.

  [[paper](https://arxiv.org/abs/2203.12709)]

  [[code](https://github.com/uva-nlp/flat)]

- Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju Hwang, Jinwoo Shin:

  Consistency Regularization for Adversarial Robustness.

  [[paper](https://arxiv.org/abs/2103.04623)]

  [[code](https://github.com/alinlab/consistency-adversarial)]

- Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon:

  Adversarial Robustness in Multi-Task Learning: Promises and Illusions.

  [[paper](https://arxiv.org/abs/2110.15053)]

  [[code](https://github.com/yamizi/taskaugment)]

- Yuan Yang, James Clayton Kerce, Faramarz Fekri:
  LOGICDEF: An Interpretable Defense Framework against Adversarial Examples via Inductive Scene Graph Reasoning.

  [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20865/20624)]

  [[code](https://github.com/gblackout/logic-adversarial-defense)]

- Jinghui Chen, Yu Cheng, Zhe Gan, Quanquan Gu, Jingjing Liu:
  Efficient Robust Training via Backward Smoothing.

  [[paper](https://arxiv.org/abs/2010.01278)]

  [[code](https://github.com/jinghuichen/BackwardSmoothing)]

- Ruoxin Chen, Jie Li, Junchi Yan, Ping Li, Bin Sheng:

  Input-Specific Robustness Certification for Randomized Smoothing.

  [[paper](https://arxiv.org/abs/2112.12084)]

  [[code](https://github.com/roy-ch/Input-Specific-Certification)]

- Mikhail Pautov, Nurislam Tursynbek, Marina Munkhoeva, Nikita Muravev, Aleksandr Petiushko, Ivan V. Oseledets:
  CC-CERT: A Probabilistic Approach to Certify General Robustness of Neural Networks.

  [[paper](https://arxiv.org/abs/2109.10696)]

  [[code](https://github.com/mikhailpautov/cccert)]

### CVPR 2022

#### Attack

- Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Fuchun Sun, Bo Zhang, Xiaolin Hu:
  Adversarial Texture for Fooling Person Detectors in the Physical World.

  [[paper](https://arxiv.org/abs/2203.03373)]

  [[code](https://github.com/WhoTHU/Adversarial_Texture)]

- Linjun Zhou, Peng Cui, Xingxuan Zhang, Yinan Jiang, Shiqiang Yang:

  Adversarial Eigen Attack on BlackBox Models.

  [[paper](https://arxiv.org/abs/2009.00097)]

- Qiuling Xu, Guanhong Tao, Xiangyu Zhang:
  Bounded Adversarial Attack on Deep Content Features.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Bounded_Adversarial_Attack_on_Deep_Content_Features_CVPR_2022_paper.pdf)]

  [[code](https://github.com/qiulingxu/D2B)]

- Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash:

  Backdoor Attacks on Self-Supervised Learning.

  [[paper](https://arxiv.org/abs/2105.10123)]

  [[code](https://github.com/UMBCvision/SSL-Backdoor)]

- Binghui Wang, Youqi Li, Pan Zhou:
  Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees.

  [[paper](https://arxiv.org/abs/2205.03546)]

  [[code](https://github.com/Metaoblivion/Bandit_GNN_Attack)]

- Yan Feng, Baoyuan Wu, Yanbo Fan, Li Liu, Zhifeng Li, Shu-Tao Xia:
  Boosting Black-Box Attack with Partially Transferred Conditional Adversarial Distribution.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Feng_Boosting_Black-Box_Attack_With_Partially_Transferred_Conditional_Adversarial_Distribution_CVPR_2022_paper.pdf)]

  [[code](https://github.com/Kira0096/CGATTACK)]

- Zhenting Wang, Juan Zhai, Shiqing Ma:

  BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning.

  [[paper](https://arxiv.org/abs/2205.13383)]

  [[code](https://github.com/RU-System-Software-and-Security/BppAttack)]

- Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang:

  Cross-Modal Transferable Adversarial Attacks from Images to Videos. 

  [[paper](https://arxiv.org/abs/2112.05379)]

- Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Huazhu Fu, Wei Feng, Yang Liu, Song Wang:
  Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection.

  [[paper](https://arxiv.org/abs/2009.09258)]

  [[code](https://github.com/tsingqguo/jadena)]

- Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, Howon Kim:

  DTA: Physical Camouflage Attacks using Differentiable Transformation Network.

  [[paper](https://arxiv.org/abs/2203.09831)]

  [[code](https://github.com/islab-ai/dta-cvpr2022)]

- Wenxuan Wang, Xuelin Qian, Yanwei Fu, Xiangyang Xue:
  DST: Dynamic Substitute Training for Data-free Black-box Attack.

  [[paper](https://arxiv.org/abs/2204.00972)]

- Xiaoqian Xu, Pengxu Wei, Weikai Chen, Yang Liu, Mingzhi Mao, Liang Lin, Guanbin Li:
  Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Dual_Adversarial_Adaptation_for_Cross-Device_Real-World_Image_Super-Resolution_CVPR_2022_paper.pdf)]

  [[code](https://github.com/lonelyhope/DADA)]

- Sivapriya Vellaichamy, Matthew Hull, Zijie J. Wang, Nilaksh Das, Sheng-Yun Peng, Haekyu Park, Duen Horng (Polo) Chau:
  DetectorDetective: Investigating the Effects of Adversarial Examples on Object Detectors.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Vellaichamy_DetectorDetective_Investigating_the_Effects_of_Adversarial_Examples_on_Object_Detectors_CVPR_2022_paper.pdf)]

  [[code](https://github.com/poloclub/detector-detective)]

- Xuxiang Sun, Gong Cheng, Hongda Li, Lei Pei, Junwei Han:

  Exploring Effective Data for Surrogate Training Towards Black-box Attack.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Exploring_Effective_Data_for_Surrogate_Training_Towards_Black-Box_Attack_CVPR_2022_paper.pdf)]

  [[code](https://github.com/xuxiangsun/ST-Data)]

- Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie, Linlin Shen:
  Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Frequency-Driven_Imperceptible_Adversarial_Attack_on_Semantic_Similarity_CVPR_2022_paper.pdf)]

  [[code](https://github.com/LinQinLiang/SSAH-adversarial-attack)]

- Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang, Weifeng Chiu, Tao Wei, Kui Ren:

  Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models.

  [[paper](https://arxiv.org/abs/2203.01584)]

- Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao:
  FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis.

  [[paper](https://arxiv.org/abs/2112.01148)]

  [[code](https://github.com/HazardFY/FIBA)]

- Zirui Peng, Shaofeng Li, Guoxing Chen, Cheng Zhang, Haojin Zhu, Minhui Xue:

  Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations.

  [[paper](https://arxiv.org/abs/2202.08602)]

- Giulio Lovisotto, Nicole Finnie, Mauricio Munoz, Chaithanya Kumar Mummadi, Jan Hendrik Metzen:
  Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness.

  [[paper](https://arxiv.org/abs/2203.13639)]

- Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Heeseon Kim, Changick Kim:
  Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Byun_Improving_the_Transferability_of_Targeted_Adversarial_Examples_Through_Object-Based_Diverse_CVPR_2022_paper.pdf)]

  [[code](https://github.com/dreamflake/ODI)]

- Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu:
  Shape-invariant 3D Adversarial Point Clouds.

  [[paper](https://arxiv.org/abs/2203.04041)]

  [[code](https://github.com/shikiw/SI-Adv)]

- Zachary Berger, Parth Agrawal, Tian Yu Liu, Stefano Soatto, Alex Wong:
  Stereoscopic Universal Perturbations across Different Architectures and Datasets.

  [[paper](https://arxiv.org/abs/2112.06116)]

  [[code](https://github.com/alexklwong/stereoscopic-universal-perturbations)]

- Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji:
  Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon

  [[paper](https://arxiv.org/abs/2203.03818)]

  [[code](https://github.com/hncszyq/ShadowAttack)]

- Yifeng Xiong, Jiadong Lin, Min Zhang, John E. Hopcroft, Kun He:
  Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability.

  [[paper](https://arxiv.org/abs/2111.10752)]

  [[code](https://github.com/JHL-HUST/SVRE)]

- Shengshan Hu, Xiaogeng Liu, Yechao Zhang, Minghui Li, Leo Yu Zhang, Hai Jin, Libing Wu:
  Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer.

  [[paper](https://arxiv.org/abs/2203.03121)]

  [[code](https://github.com/CGCL-codes/AMT-GAN)]

- Mostafa Kahla, Si Chen, Hoang Anh Just, Ruoxi Jia:

  Label-Only Model Inversion Attacks via Boundary Repulsion.

  [[paper](https://arxiv.org/abs/2203.01925)]

  [[code](https://github.com/m-kahla/Label-Only-Model-Inversion-Attacks-via-Boundary-Repulsion)]

- Jianping Zhang, Weibin Wu, Jen-tse Huang, Yizhan Huang, Wenxuan Wang, Yuxin Su, Michael R. Lyu:

  Improving Adversarial Transferability via Neuron Attribution-based Attacks.

  [[paper](https://arxiv.org/abs/2204.00008)]

  [[code](https://github.com/jpzhang1810/NAA)]

- Chaoning Zhang, Philipp Benz, Adil Karjauv, Jae-Won Cho, Kang Zhang, In So Kweon:

  Investigating Top-k White-Box and Transferable Black-box Attack. 

  [[paper](https://arxiv.org/abs/2204.00089)]

  [[code](https://github.com/ChaoningZhang/Top-k-Transferable-Attack)] - coming soon

- Byung-Kwan Lee, Junho Kim, Yong Man Ro:
  Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network.

  [[paper](https://arxiv.org/abs/2204.02738)]

  [[code](https://github.com/ByungKwanLee/Masking-Adversarial-Damage)]

- Zikui Cai, Shantanu Rane, Alejandro E. Brito, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury, M. Salman Asif:
  Zero-Query Transfer Attacks on Context-Aware Object Detectors.

  [[paper](https://arxiv.org/abs/2203.15230)]

- Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, Zhangyang Wang:

  Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free.

  [[paper](https://arxiv.org/abs/2205.11819)]

  [[code](https://github.com/VITA-Group/Backdoor-LTH)]

- Jie Zhang, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Lei Zhang, Chao Wu:
  Towards Efficient Data Free Blackbox Adversarial Attack.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Towards_Efficient_Data_Free_Black-Box_Adversarial_Attack_CVPR_2022_paper.pdf)]

- Transferable Sparse Adversarial Attack

  [[paper](https://arxiv.org/abs/2105.14727)]

  [[code](https://github.com/shaguopohuaizhe/TSAA)]

- Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu:

  Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks.

  [[paper](https://arxiv.org/abs/2111.12965)]

  [[code](https://github.com/Unispac/Subnet-Replacement-Attack)]

- Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong, Dakui Wang, Kaitai Liang:
  DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_DEFEAT_Deep_Hidden_Feature_Backdoor_Attacks_by_Imperceptible_Perturbation_and_CVPR_2022_paper.pdf)]

- Shuai Jia, Chao Ma, Taiping Yao, Bangjie Yin, Shouhong Ding, Xiaokang Yang:
  Exploring Frequency Adversarial Attacks for Face Forgery Detection.

  [[paper](https://arxiv.org/abs/2203.15674)]

- Yunjian Zhang, Yanwei Liu, Jinxia Liu, Jingbo Miao, Antonios Argyriou, Liming Wang, Zhen Xu:
  360-Attack: Distortion-Aware Perturbations from Perspective-Views.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_360-Attack_Distortion-Aware_Perturbations_From_Perspective-Views_CVPR_2022_paper.pdf)]

#### Defense

- Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, Xiaowei Huang:

  Enhancing Adversarial Training with Second-Order Statistics of Weights.

  [[paper](https://arxiv.org/abs/2203.06020)]

  [[code](https://github.com/Alexkael/S2O)]

- Mo Zhou, Vishal M. Patel:
  Enhancing Adversarial Robustness for Deep Metric Learning. 

  [[paper](https://arxiv.org/abs/2203.01439)]

  [[code](https://github.com/cdluminate/robdml)]

- Ozan Özdenizci, Robert Legenstein:
  Improving Robustness Against Stealthy Weight Bit-Flip Attacks by Output Code Matching.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Ozdenizci_Improving_Robustness_Against_Stealthy_Weight_Bit-Flip_Attacks_by_Output_Code_CVPR_2022_paper.pdf)]

  [[code](https://github.com/IGITUGraz/OutputCodeMatching)]

- Junhao Dong, Yuan Wang, Jianhuang Lai, Xiaohua Xie:
  Improving Adversarially Robust Few-shot Image Classification with Generalizable Representations.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Improving_Adversarially_Robust_Few-Shot_Image_Classification_With_Generalizable_Representations_CVPR_2022_paper.pdf)]

- Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang:
  Subspace Adversarial Training. 

  [[paper](https://arxiv.org/abs/2111.12229)]

  [[code](https://github.com/nblt/Sub-AT)]

- Jiang Liu, Alexander Levine, Chun Pong Lau, Rama Chellappa, Soheil Feizi:
  Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection. 

  [[paper](https://arxiv.org/abs/2112.04532)]

  [[code](https://github.com/joellliu/SegmentAndComplete)]

- Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, Jue Wang:
  Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection

  [[paper](https://arxiv.org/abs/2203.12208)]

  [[code](https://github.com/liangchen527/SLADD)]

- Zhaoyu Chen, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Wenqiang Zhang:
  Towards Practical Certifiable Patch Defense with Vision Transformer.

  [[paper](https://arxiv.org/abs/2203.08519)]

- Ye Liu, Yaya Cheng, Lianli Gao, Xianglong Liu, Qilong Zhang, Jingkuan Song:

  Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack.

  [[paper](https://arxiv.org/abs/2203.05154)]

  [[code](https://github.com/liuye6666/adaptive_auto_attack)]

- Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, Xiaochun Cao:

  LAS-AT: Adversarial Training with Learnable Attack Strategy.

  [[paper](https://arxiv.org/abs/2203.06616)]

  [[code](https://github.com/jiaxiaojunQAQ/LAS-AT)]

- Kaidong Li, Ziming Zhang, Cuncong Zhong, Guanghui Wang:
  Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients.

  [[paper](https://arxiv.org/abs/2203.15245)]

  [[code](https://zhang-vislab.github.io)]

- Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He, Deliang Fan, Chaitali Chakrabarti:

  ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning.

  [[paper](https://arxiv.org/abs/2205.04007)]

  [[code](https://github.com/zlijingtao/ResSFL)]

- Yi Yu, Wenhan Yang, Yap-Peng Tan, Alex C. Kot:
  Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Towards_Robust_Rain_Removal_Against_Adversarial_Attacks_A_Comprehensive_Benchmark_CVPR_2022_paper.pdf)]

  [[code](https://github.com/yuyi-sd/Robust_Rain_Removal)]

- Jiakai Wang, Zixin Yin, Pengfei Hu, Aishan Liu, Renshuai Tao, Haotong Qin, Xianglong Liu, Dacheng Tao:

  Defensive Patches for Robust Recognition in the Physical World.

  [[paper](https://arxiv.org/abs/2204.06213)]

  [[code](https://github.com/nlsde-safety-team/DefensivePatch)]

- Theodoros Tsiligkaridis, Jay Roberts:
  Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training

  [[paper](https://arxiv.org/pdf/2012.12368.pdf)]

  [[code](https://github.com/TheoT1/FW-AT-Adapt)]

- Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen, Z. Morley Mao:
  On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles.

  [[paper](https://arxiv.org/abs/2201.05057)]

  [[code](https://github.com/zqzqz/AdvTrajectoryPrediction)]

- Prithviraj Dhar, Amit Kumar, Kirsten Kaplan, Khushi Gupta, Rakesh Ranjan, Rama Chellappa:
  EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection using Periocular Images.

  [[paper](https://arxiv.org/abs/2112.11610)]

#### Others

- Qibing Ren, Qingquan Bao, Runzhong Wang, Junchi Yan:
  Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond.

  [[paper](https://runzhong.wang/files/cvpr22robustGM.pdf)]

  [[code](https://github.com/thinklab-sjtu/robustmatch)]

- Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, Tie-Yan Liu:
  Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart.

  [[paper](https://arxiv.org/abs/2105.14785)]

  [[code](https://github.com/P2333/Rectified-Rejection)]

- Kwang In Kim:
  Robust Combination of Distributed Gradients Under Adversarial Perturbations.

  [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Robust_Combination_of_Distributed_Gradients_Under_Adversarial_Perturbations_CVPR_2022_paper.pdf)]

- Yingzhi Tang, Yue Qian, Qijian Zhang, Yiming Zeng, Junhui Hou, Xuefei Zhe:

  WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation.

  [[paper](https://arxiv.org/abs/2203.12917)]

  [[code](https://github.com/yztang4/WarpingGAN)]

- Ganesh Del Grosso, Hamid Jalalzai, Georg Pichler, Catuscia Palamidessi, Pablo Piantanida:
  Leveraging Adversarial Examples to Quantify Membership Information Leakage.

  [[paper](https://arxiv.org/abs/2203.09566)]

  [[code](https://github.com/ganeshdg95/Leveraging-Adversarial-Examples-to-Quantify-Membership-Information-Leakage)]

### ACM MM 2022

- Siyuan Liang, Aishan Liu, Jiawei Liang, Longkang Li, Yang Bai, Xiaochun Cao:
  Imitated Detectors: Stealing Knowledge of Black-box Object Detectors.

  [[paper](https://scst.sysu.edu.cn/docs/20220718132716248974.pdf)]

  [[code](https://github.com/LiangSiyuan21/Imitated-Detectors)]

- Yuxuan Wang, Jiakai Wang, Zixin Yin, Ruihao Gong, Jingyi Wang, Aishan Liu, Xianglong Liu:
  Generating Transferable Adversarial Examples against Vision Transformers.

  [paper] not found yet

### ECCV 2022

#### Attack

- Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xianglong Liu, Jian Zhang, Jingkuan Song:
  Frequency Domain Model Augmentation for Adversarial Attack.

  [[paper](https://arxiv.org/abs/2207.05382)]

  [[code](https://github.com/yuyang-long/SSA)]

- Ziyi Dong, Pengxu Wei, Liang Lin:
  Adversarially-Aware Robust Object Detector.

  [[paper](https://arxiv.org/abs/2207.06202)]

  [[code](https://github.com/7eu7d7/RobustDet)]

- Jenny Schmalfuss, Philipp Scholze, Andrés Bruhn:
  A Perturbation-Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow.

  [[paper](https://arxiv.org/abs/2203.13214)]

  [[code](https://github.com/cv-stuttgart/PCFA)]

- Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, Xiangyu Zhang:
  Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches.

  [[paper](https://arxiv.org/abs/2207.04718)]

- Zhaoyu Chen, Bo Li, Shuang Wu, Jianghe Xu, Shouhong Ding, Wenqiang Zhang:

  Shape Matters: Deformable Patch Attack.

  [[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640522.pdf)]

- Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon, Koushik Sen:
  LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity.

  [[paper](https://arxiv.org/abs/2207.13129)]

  [[code](https://github.com/Framartin/lgv-geometric-transferability)]

#### Defense

#### Others

### ICLR 2022

### NIPS 2022

#### Attack

- Anshuman Chhabra, Ashwin Sekhari, Prasant Mohapatra:
  On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses.

  [[paper](https://arxiv.org/abs/2210.01940)]

  [[code]()]

- Sizhe Chen, Zhehao Huang, Qinghua Tao, Yingwen Wu, Cihang Xie, Xiaolin Huang:
  Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks.

  [[paper](https://arxiv.org/abs/2205.12134)]

  [[code](https://github.com/Sizhe-Chen/AAA)]

- Abhishek Aich, Calvin-Khang Ta, Akash Gupta, Chengyu Song, Srikanth V. Krishnamurthy, M. Salman Asif, Amit Roy-Chowdhury:
  GAMA: Generative Adversarial Multi-Object Scene Attacks.

  [[paper](https://arxiv.org/abs/2209.09502)]

  [[code](https://github.com/abhishekaich27/GAMA-pytorch)]

- Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan:

  BadPrompt: Backdoor Attacks on Continuous Prompts.

  [[paper](https://arxiv.org/abs/2211.14719)]

  [[code](https://github.com/papersPapers/BadPrompt)]

- Patrick O'Reilly, Andreas Bugler, Keshav Bhandari, Max Morrison, Bryan Pardo:
  VoiceBlock: Privacy through Real-Time Adversarial Attacks with Audio-to-Audio Models.

  [[paper](https://openreview.net/pdf?id=8gQEmEgWAkc)]

  [[code](https://github.com/voiceboxneurips/voicebox)]

- Zihan Liu, Yun Luo, Lirong Wu, Zicheng Liu, Stan Z. Li:
  Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias.

  [[paper](https://arxiv.org/abs/2304.00010)]

  [[code](https://github.com/Zihan-Liu-00/GraD--NeurIPS22)]

- Yucheng Shi, Yahong Han, Yu-an Tan, Xiaohui Kuang:
  Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal.

  [[paper](https://arxiv.org/abs/2112.03492)]

  [[code](https://github.com/shiyuchengTJU/PAR)]

- Haoyang Li, Shimin Di, Lei Chen:
  Revisiting Injective Attacks on Recommender Systems.

  [[paper](https://arxiv.org/abs/2008.04876)]

  [[code](https://github.com/graytowne/revisit_adv_rec)]

- Weixia Zhang, Dingquan Li, Xiongkuo Min, Guangtao Zhai, Guodong Guo, Xiaokang Yang, Kede Ma:
  Perceptual Attacks of No-Reference Image Quality Models with Human-in-the-Loop

  [[paper](https://arxiv.org/abs/2210.00933)]

- Khoa D. Doan, Yingjie Lao, Ping Li:
  Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class.

  [[paper](https://arxiv.org/abs/2210.09194)]

  [[code](https://github.com/khoadoan106/backdoor_attacks)]

- Henger Li, Xiaolin Sun, Zizhan Zheng:
  Learning to Attack Federated Learning: A Model-based Reinforcement Learning Attack Framework.

  [[paper](https://openreview.net/pdf?id=4OHRr7gmhd4)]

  [[code](https://github.com/SliencerX/Learning-to-Attack-Federated-Learning)]

- Zeyu Qin, Yanbo Fan, Yi Liu, Li Shen, Yong Zhang, Jue Wang, Baoyuan Wu:
  Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation.

  [[paper](https://arxiv.org/abs/2210.05968)]

  [[code](https://github.com/Alan-Qin/Transfer_attack_RAP)]

- Shuai Jia, Bangjie Yin, Taiping Yao, Shouhong Ding, Chunhua Shen, Xiaokang Yang, Chao Ma:

  Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition.

  [[paper](https://arxiv.org/abs/2210.06871)]

- Zikui Cai, Chengyu Song, Srikanth Krishnamurthy, Amit Roy-Chowdhury, M. Salman Asif

  Blackbox Attacks via Surrogate Ensemble Search.

  [[paper](https://arxiv.org/abs/2208.03610)]

  [[code](https://github.com/CSIPlab/BASES)]

- Shengming Yuan, Qilong Zhang, Lianli Gao, Yaya Cheng, Jingkuan Song:

  Natural Color Fool: Towards Boosting Black-box Unrestricted Attacks.

  [[paper](https://arxiv.org/abs/2210.02041)]

  [[code](https://github.com/VL-Group/Natural-Color-Fool)]

- Chenghao Sun, Yonggang Zhang, Chaoqun Wan, Qizhou Wang, Ya Li, Tongliang Liu, Bo Han, Xinmei Tian:
  Towards Lightweight Black-Box Attack Against Deep Neural Networks.

  [[paper](https://arxiv.org/abs/2209.14826)]

  [[code](https://github.com/sunch-ustc/Error_TransFormer/tree/ETF)]

- Fan Liu, Hao Liu, Wenzhao Jiang:
  Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models.

  [[paper](https://arxiv.org/abs/2210.02447)]

  [[code](https://github.com/usail-hkust/Adv-ST)]

- Shuwen Chai, Jinghui Chen:
  One-shot Neural Backdoor Erasing via Adversarial Weight Masking.

  [[paper](https://github.com/jinghuichen/AWM)]

  [[code](https://github.com/jinghuichen/AWM)]

- Yibo Miao, Yinpeng Dong, Jun Zhu, Xiao-Shan Gao:
  Isometric 3D Adversarial Examples in the Physical World.

  [[paper](https://arxiv.org/abs/2210.15291)]

#### Defense

- Yunrui Yu, Xitong Gao, Cheng-Zhong Xu:

  MORA: Improving Ensemble Robustness Evaluation with Model Reweighing Attack.

  [[paper](https://arxiv.org/abs/2211.08008)]

  [[code](https://github.com/lafeat/mora)] not open source yet

- Yunjuan Wang, Enayat Ullah, Poorya Mianjy, Raman Arora:
  Adversarial Robustness is at Odds with Lazy Training.

  [[paper](https://arxiv.org/abs/2207.00411)]

- Xiyuan Li, Zou Xin, Weiwei Liu:

  Defending Against Adversarial Attacks via Neural Dynamic System.

  [[paper](https://arxiv.org/abs/1803.05123)]

- Zhuoer Xu, Guanghui Zhu, Changhua Meng, Shiwen Cui, Zhenzhe Ying, Weiqiang Wang, Ming Gu, Yihua Huang:
  A2: Efficient Automated Attacker for Boosting Adversarial Training.

  [[paper](https://arxiv.org/abs/2210.03543)]

  [[code](https://github.com/alipay/A2-efficient-automated-attacker-for-boosting-adversarial-training)]

- Ruisi Cai, Zhenyu Zhang, Tianlong Chen, Xiaohan Chen, Zhangyang Wang:
  Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets.

  [[paper](https://openreview.net/pdf?id=TItRK4VP9X2)]

  [[code](https://github.com/VITA-Group/Random-Shuffling-BackdoorDetect)]

- Tian Yu Liu, Yu Yang, Baharan Mirzasoleiman:

  Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attack.

  [[paper](https://arxiv.org/abs/2208.10224)]

  [[code](https://github.com/tianyu139/friendly-noise)]

- Jianan Zhou, Jianing Zhu, Jingfeng Zhang, Tongliang Liu, Gang Niu, Bo Han, Masashi Sugiyama:
  Adversarial Training with Complementary Labels: On the Benefit of Gradually Informative Attacks.

  [[paper](https://arxiv.org/abs/2211.00269)]

  [[code](https://github.com/RoyalSkye/ATCL)]

- Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, Zhangyang Wang:
  Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork.

  [[paper](https://arxiv.org/abs/2210.06428)]

  [[code](https://github.com/VITA-Group/Trap-and-Replace-Backdoor-Defense)]

- Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Furong Huang:
  Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning.

  [[paper](https://arxiv.org/abs/2210.05927)]

  [[code](https://github.com/umd-huang-lab/WocaR-RL)]

- Sihui Dai, Saeed Mahloujifar, Prateek Mittal:

  Formulating Robustness Against Unforeseen Attacks.

  [[paper](https://arxiv.org/abs/2204.13779)]

  [[code](https://github.com/inspire-group/variation-regularization)]

- Anna Kuzina, Max Welling, Jakub M. Tomczak:
  Alleviating Adversarial Attacks on Variational Autoencoders with MCMC.

  [[paper](https://arxiv.org/abs/2203.09940)]

  [[code](https://github.com/AKuzina/defend_vae_mcmc)]

- Daniel M. Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter Schmidt-Nielsen, Tao Lin, Adam Scherlis, Noa Nabeshima, Ben Weinstein-Raun, Daniel de Haas, Buck Shlegeris, Nate Thomas:
  Adversarial training for high-stakes reliability.

  [[paper](https://arxiv.org/abs/2205.01663)]

- Yue Xing, Qifan Song, Guang Cheng:
  Phase Transition from Clean Training to Adversarial Training.

  [[paper](https://openreview.net/attachment?id=gwsnBjNcVEe&name=supplementary_material)]

- Yue Xing, Qifan Song, Guang Cheng:

  Why Do Artificially Generated Data Help Adversarial Robustness.

  [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/065e259a1d2d955e63b99aac6a3a3081-Paper-Conference.pdf)]

- Ling Liang, Kaidi Xu, Xing Hu, Lei Deng, Yuan Xie:

  Toward Robust Spiking Neural Network Against Adversarial Perturbation.

  [[paper](https://openreview.net/pdf?id=Ncyc0JS7Q16)]

  [[code](https://github.com/liangling76/certify_snn)]

- Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong:
  MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples.

  [[paper](https://arxiv.org/abs/2210.01111)]

  [[code](https://github.com/quwenjie/MultiGuard)]

- Jianhao Ding, Tong Bu, Zhaofei Yu, Tiejun Huang, Jian K. Liu:
  SNN-RAT: Robustness-enhanced Spiking Neural Network through Regularized Adversarial Training.

  [[paper](https://openreview.net/pdf?id=xwBdjfKt7_W)]

  [[code](https://github.com/putshua/SNN-RAT)]

- Zonghan Yang, Tianyu Pang, Yang Liu:

  A Closer Look at the Adversarial Robustness of Deep Equilibrium Models.

  [[paper](https://openreview.net/pdf?id=_WHs1ruFKTD)]

  [[code](https://github.com/minicheshire/deq-white-box-robustness)]

- Pau de Jorge Aranda, Adel Bibi, Riccardo Volpi, Amartya Sanyal, Philip H. S. Torr, Grégory Rogez, Puneet K. Dokania:
  Make Some Noise: Reliable and Efficient Single-Step Adversarial Training.

  [[paper](https://arxiv.org/abs/2202.01181)]

  [[code](https://github.com/pdejorge/N-FGSM)]

- Chen Chen, Yuchen Liu, Xingjun Ma, Lingjuan Lyu:
  CalFAT: Calibrated Federated Adversarial Training with Label Skewness.

  [[paper](https://arxiv.org/abs/2205.14926)]

  [[code](https://github.com/cc233/CalFAT)]

- Xiaofeng Mao, Yuefeng Chen, Ranjie Duan, Yao Zhu, Gege Qi, Shaokai Ye, Xiaodan Li, Rong Zhang, Hui Xue:

  Enhance the Visual Representation via Discrete Adversarial Training.

  [[paper](https://arxiv.org/abs/2209.07735)]

  [[code](https://github.com/alibaba/easyrobust)]

- Mazda Moayeri, Kiarash Banihashem, Soheil Feizi:
  Explicit Tradeoffs between Adversarial and Natural Distributional Robustness.

  [[paper](https://arxiv.org/abs/2209.07592)]

- Chengyu Dong, Liyuan Liu, Jingbo Shang:
  Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting.

  [[paper](https://openreview.net/forum?id=9_O9mTLYJQp)]

- Omar Montasser, Steve Hanneke, Nati Srebro:
  Adversarially Robust Learning: A Generic Minimax Optimal Learner and Characterization.

  [[paper](https://arxiv.org/abs/2209.07369)]

- Avrim Blum, Omar Montasser, Greg Shakhnarovich, Hongyang Zhang:

  Boosting Barely Robust Learners: A New Perspective on Adversarial Robustness.

  [[paper](https://openreview.net/forum?id=s776AhRFm67)]

- Jiancong Xiao, Yanbo Fan, Ruoyu Sun, Jue Wang, Zhi-Quan Luo:

  Stability Analysis and Generalization Bounds of Adversarial Training.

  [[paper](https://arxiv.org/abs/2210.00960)]

  [[code](https://github.com/JiancongXiao/Stability-of-Adversarial-Training)]

- Sravanti Addepalli, Samyak Jain, Venkatesh Babu R.:
  Efficient and Effective Augmentation Strategy for Adversarial Training.

  [[paper](https://arxiv.org/abs/2210.15318)]

  [[code](https://github.com/val-iisc/DAJAT)]

- Minjing Dong, Xinghao Chen, Yunhe Wang, Chang Xu:
  Random Normalization Aggregation for Adversarial Defense.

  [[paper](https://openreview.net/forum?id=K4W92FUXSF9)]

  [[code](https://github.com/UniSerj/Random-Norm-Aggregation)]

- Chih-Hui Ho, Nuno Vasconcelos:
  DISCO: Adversarial Defense with Local Implicit Functions.

  [[paper](https://openreview.net/forum?id=vgIz0emVTAd)]

  [[code](https://github.com/chihhuiho/disco)]

- Sen Cui, Jingfeng Zhang, Jian Liang, Bo Han, Masashi Sugiyama, Changshui Zhang:

  Synergy-of-Experts: Collaborate to Improve Adversarial Robustness.

  [[paper](https://openreview.net/forum?id=tuC6teLFZD)]

  [[code](https://github.com/cuis15/synergy-of-experts)]

- Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu:
  ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints.

  [[paper](https://arxiv.org/abs/2210.03895)]

  [[code](https://github.com/Heathcliff-saku/ViewFool_)]

- Bohang Zhang, Du Jiang, Di He, Liwei Wang:

  Rethinking Lipschitz Neural Networks and Certified Robustness: A Boolean Function Perspective.

  [[paper](https://arxiv.org/abs/2210.01787)]

  [[code](https://github.com/zbh2047/SortNet)]

#### Others

- Maura Pintor, Luca Demetrio, Angelo Sotgiu, Ambra Demontis, Nicholas Carlini, Battista Biggio, Fabio Roli:
  Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples.

  [[paper](https://arxiv.org/abs/2106.09947)]

  [[code](https://github.com/pralab/IndicatorsOfAttackFailure)]

- Lue Tao, Lei Feng, Hongxin Wei, Jinfeng Yi, Sheng-Jun Huang, Songcan Chen:
  Can Adversarial Training Be Manipulated By Non-Robust Features?

  [[paper](https://arxiv.org/abs/2201.13329)]

  [[code](https://github.com/TLMichael/Hypocritical-Perturbation)]

- Idan Attias, Steve Hanneke, Yishay Mansour:

  A Characterization of Semi-Supervised Adversarially Robust PAC Learnability.

  [[paper](https://arxiv.org/abs/2202.05420)]

- Li-Cheng Lan, Huan Zhang, Ti-Rong Wu, Meng-Yu Tsai, I-Chen Wu, Cho-Jui Hsieh:
  Are AlphaZero-like Agents Robust to Adversarial Perturbations?

  [[paper](https://arxiv.org/abs/2211.03769)]

  [[code](https://github.com/lan-lc/adversarial_example_of_Go)]

- Joan Puigcerver, Rodolphe Jenatton, Carlos Riquelme, Pranjal Awasthi, Srinadh Bhojanapalli:

  On the Adversarial Robustness of Mixture of Experts.

  [[paper](https://arxiv.org/abs/2210.10253)]

- Roland S. Zimmermann, Wieland Brendel, Florian Tramèr, Nicholas Carlini:
  Increasing Confidence in Adversarial Robustness Evaluations.

  [[paper](https://arxiv.org/abs/2206.13991)]

  [[code](https://zimmerrol.github.io/active-tests/)]

- Nikolaos Tsilivis, Julia Kempe:

  What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?

  [[paper](https://arxiv.org/abs/2210.05577)]

  [[code](https://github.com/Tsili42/adv-ntk)]


## Benchmark

* [RobustBench: A standardized benchmark for adversarial robustness](https://robustbench.github.io/)


## Distinguished Researchers & Teams
Distinguished ***TODO*** researchers who have published +3 papers which have a major impact on the field of ***TODO*** and are still active in the field of ***TODO***. (Names listed in no particular order.)

* TODO
